{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Bert-base TensorFlow 2.0\n\nThis kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https://www.kaggle.com/corochann/google-quest-first-data-introduction), [getting started](https://www.kaggle.com/phoenix9032/get-started-with-your-questions-eda-model-nn) & [another getting started](https://www.kaggle.com/hamditarek/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using ~~TensorFow Hub~~ Huggingface transformer. <br><br>\n\n---\n**Update 1 (Commit 7):**\n* removing penultimate dense layer; now there's only one dense layer (output layer) for fine-tuning\n* using BERT's sequence_output instead of pooled_output as input for the dense layer\n---\n\n**Update 2 (Commit 8):**\n* adjusting `_trim_input()` --- now have a q_max_len and a_max_len, instead of 'keeping the ratio the same' while trimming.\n* **importantly:** now also includes question_title for the input sequence\n---\n\n**Update 3 (Commit 9)**\n<br><br>*A lot of experiments can be made with the title + body + answer sequence. Feel free to look into e.g. (1) inventing new tokens (add it to '../input/path-to-bert-folder/assets/vocab.txt'), (2) keeping \\[SEP\\] between title and body but modify `_get_segments()`, (3) using the \\[PAD\\] token, or (4) merging title and body without any kind of separation. In this commit I'm doing (2). I also tried (3) offline, and they both perform better than in commit 8, in terms of validation rho.*<br>\n\n* ignoring first \\[SEP\\] token in `_get_segments()`.\n\n---\n\n**Update 4 (Commit 11)**\n* **Now using Huggingface transformer instead of TFHub** (note major changes in the code). This creates the possibility to easily try out different architectures like XLNet, Roberta etc. As well as easily outputting the hidden states of the transformer.\n* two separate inputs (title+body and answer) for BERT\n* removed snapshot average (now only using last (third) epoch). This will likely decrease performance, but it's not feasible to use ~ 5 x 4 models for a single bert prediction in practice. \n* only training for 2 epochs instead of 3 (to manage 2h limit)\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"import subprocess\nfrom ast import literal_eval\n\ndef run(command):\n    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n    out, err = process.communicate()\n    print(out.decode('utf-8').strip())\n\nprint('# CPU')\nrun('cat /proc/cpuinfo | egrep -m 1 \"^model name\"')\nrun('cat /proc/cpuinfo | egrep -m 1 \"^cpu MHz\"')\nrun('cat /proc/cpuinfo | egrep -m 1 \"^cpu cores\"')\n\nprint('# RAM')\nrun('cat /proc/meminfo | egrep \"^MemTotal\"')\n\nprint('# GPU')\nrun('lspci | grep VGA')\n\nprint('# OS')\nrun('uname -a')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses > /dev/null\n\nimport sys\nsys.path.insert(0, \"../input/transformers/\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n# import tensorflow_hub as hub\nimport tensorflow as tf\n# import bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nimport seaborn as sns\nimport string\nimport re    #for regex\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\n\nHAS_ANS = False\n\n# BERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\n# tokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n\nBERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\ndel df_train['question_body_critical']\n\nif HAS_ANS:\n    output_categories = list(df_train.columns[11:])\n    input_categories = list(df_train.columns[[1,2,5]])\nelse:\n    # ISSA\n    output_categories = list(df_train.columns[11:31])\n    input_categories = list(df_train.columns[[1,2,5]])\n\nTARGET_COUNT = len(output_categories)\n\nprint('\\noutput TARGET_COUNT:\\n\\t', TARGET_COUNT)\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(df_train.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, axes = plt.subplots(5, 4, figsize=(13, 10))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 10)\n\nfor i, col in enumerate(output_categories):\n    ax = axes[i]\n    sns.distplot(df_train[col], label=col, kde=False, bins=bins, ax=ax)\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nsns.heatmap(df_train[output_categories].corr(), ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_train.copy()\ndef char_count(s):\n    return len(s)\n\ndef word_count(s):\n    return s.count(' ')\n\ntrain['question_title_n_chars'] = train['question_title'].apply(char_count)\ntrain['question_title_n_words'] = train['question_title'].apply(word_count)\ntrain['question_body_n_chars'] = train['question_body'].apply(char_count)\ntrain['question_body_n_words'] = train['question_body'].apply(word_count)\n\ntrain['question_body_n_chars'].clip(0, 5000, inplace=True)\ntrain['question_body_n_words'].clip(0, 5000, inplace=True)\ntrain['question_title_n_chars'].clip(0, 120, inplace=True)\ntrain['question_title_n_words'].clip(0, 120, inplace=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['question_title_n_chars'], label='chars', ax=axes[0])\nsns.distplot(train['question_title_n_words'], label='words', ax=axes[0])\naxes[0].set_xlabel('Question Title')\naxes[0].legend()\nsns.distplot(train['question_body_n_words'], label='chars', ax=axes[1])\nsns.distplot(train['question_body_n_chars'], label='words', ax=axes[1])\naxes[1].set_xlabel('Question Body')\naxes[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"question_body_n_punctuations\"] = train[\"question_body\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntrain[\"question_body_n_duplicates\"] = train[\"question_body\"].apply(lambda x: len(str(x).split()) - len(set(str(x).split())) )\ntrain[\"question_body_rate_duplicates\"] = train[\"question_body\"].apply(lambda x: (len(str(x).split()) - len(set(str(x).split()))) / (len(str(x).split())+1) )\ntrain[\"question_body_n_sentences\"] = train[\"question_body\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import cdist\n\ndef calc_corr(df, x_cols, y_cols):\n    arr1 = df[x_cols].T.values\n    arr2 = df[y_cols].T.values\n    corr_df = pd.DataFrame(1 - cdist(arr2, arr1, metric='correlation'), index=y_cols, columns=x_cols)\n    return corr_df\n\nnumber_feature_cols = ['question_title_n_chars', 'question_title_n_words', 'question_body_n_chars', 'question_body_n_words',\n                      'question_body_n_punctuations', 'question_body_n_duplicates', 'question_body_rate_duplicates', 'question_body_n_sentences']\n# train[number_feature_cols].corrwith(train[target_cols], axis=0)\n\ncorr_df = calc_corr(train, output_categories, number_feature_cols)\nfig, ax = plt.subplots(figsize=(6, 2))\nsns.heatmap(corr_df, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ntrain['q_body_polarity'] = train['question_body'].apply(pol)\ntrain['q_body_subjectivity'] = train['question_body'].apply(sub)\n\ntrain[['question_body', 'category', 'q_body_polarity', 'q_body_subjectivity']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Polarity and subjectivity plot\nplt.figure(figsize=(12,5))\ng = sns.scatterplot(x='q_body_polarity', y='q_body_subjectivity', \n                    data=train)\n#g.set_title(\"Sentiment Analysis (Polarity x Subjectivity) of question_body by 'Category' Feature\", fontsize=16)\ng.set_xlabel(\"Polarity\",fontsize=18)\ng.set_ylabel(\"Subjectivity \",fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_feature_cols = ['q_body_polarity', 'q_body_subjectivity']\n# train[number_feature_cols].corrwith(train[target_cols], axis=0)\n\ncorr_df = calc_corr(train, output_categories, number_feature_cols)\nfig, ax = plt.subplots(figsize=(6, 1))\nsns.heatmap(corr_df, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n\n*update 4:* credits to [Minh](https://www.kaggle.com/dathudeptrai) for this implementation. If I'm not mistaken, it could be used directly with other Huggingface transformers too! Note that due to the 2 x 512 input, it will require significantly more memory when finetuning BERT."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n        \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        if HAS_ANS:\n            t, q, a = instance.question_title, instance.question_body, instance.answer\n        else:\n            t, q, a = instance.question_title, instance.question_body, instance.question_title\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Create model\n\n~~`compute_spearmanr()`~~ `mean_squared_error` is used to compute the competition metric for the validation set\n<br><br>\n`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(\n        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(TARGET_COUNT, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Obtain inputs and targets, as well as the indices of the train/validation splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"output_categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nfrom sklearn.metrics import confusion_matrix\ndef print_evaluation_metrics(y_true, y_pred, label):\n    ### For regression\n    print('mean_absolute_error',label,':', sklearn.metrics.mean_absolute_error(y_true, y_pred))\n    print('mean_squared_error',label,':', sklearn.metrics.mean_squared_error(y_true, y_pred))\n    print('r2 score',label,':', sklearn.metrics.r2_score(y_true, y_pred))\n#     print('max_error',label,':', sklearn.metrics.max_error(y_true, y_pred))\n    ### FOR Classification\n#     print('accuracy_score',label,':', sklearn.metrics.accuracy_score(y_true, y_pred))\n#     print('f1_score',label,':', sklearn.metrics.f1_score(y_true, y_pred))\n#     print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n#     print('average_precision_score',label,':', sklearn.metrics.average_precision_score(y_true, y_pred))\n#     print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n#     confusion_matrix(y_true, y_pred).ravel()\n\nprint_evaluation_metrics([1,0], [0.9,0.1], 'on #'+str(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss function selection\nRegression problem between 0 and 1, so binary_crossentropy and mean_absolute_error seem good.\n\nHere are the explanations: https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_sample_count = 4700 # 4860\ntraining_epochs = 3 # 3\nrunning_folds = 1 # 2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for LR in np.arange(1e-5, 9e-5, 1e-5).tolist():\n    print('>>>>>>>>>>')\n    print('LR=', LR)\n    gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n    valid_preds = []\n    test_preds = []\n    for fold, (train_idx, valid_idx) in enumerate(gkf):\n        if fold not in range(running_folds):\n            continue\n        train_inputs = [(inputs[i][train_idx])[:training_sample_count] for i in range(len(inputs))]\n        train_outputs = (outputs[train_idx])[:training_sample_count]\n\n        print(np.array(train_inputs).shape, np.array(train_outputs).shape)\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n\n        K.clear_session()\n        model = create_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        for xx in range(training_epochs):\n            model.fit(train_inputs, train_outputs, epochs=1, batch_size=6)\n            # model.save_weights(f'bert-{fold}.h5')\n            valid_preds.append(model.predict(valid_inputs))\n            test_preds.append(model.predict(test_inputs))\n            #rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n            #print('validation score = ', rho_val)\n            print_evaluation_metrics(np.array(valid_outputs), np.array(valid_preds[-1]), 'on #'+str(xx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(valid_inputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(valid_outputs.shape, valid_preds[-1].shape)\nprint_evaluation_metrics(np.array(valid_outputs), np.array(valid_preds[-1]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":4}